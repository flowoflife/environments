Deep Learning Specialization (Coursera, Andrew Ng)
Noted by Phan Hoang Phuong
2021 Jan 28

1. Logistic Regression:

yhat = Sigmoid(wTx + b),
where Sigmoid(z) = 1/(1+e power of -z).
So z = wTx + b

Loss function L(yhat, y) = 1/2 * sum of square(yhat - y)
or it log-likelihood function: L(yhat, y) = -(ylog(yhat) + (1-y)log(1-yhat))

Cost function J(w,b) = 1/m Sum(L(yhat, y)),
 with i from 1 to m

The loss function computes the error for a single training example.

The cost function is the average of the loss functions of the entire training set.

2. Gradient Descent:

Moving cost function J(w,b) value to its global optima point

w := w - learning_rate * dJ(w,b)/dw
b := b - learning_rate * dJ(w,b)/db

dJ(w,b)/dw is partial derivative of cost function J(w,b), or the slope.
It shows how much J changed when w was changed.

dJ(w,b)/db is partial derivative of cost function J(w,b), or the slope.
It shows how much J changed when b was changed.

(note: mark of partial derivative is not d, but a squiggle symbol)

3. Logistic regression derivatives:
To update w and b, looping [i] from 1 to m,
then divide by m to get mean:
dw[i] = x[i].dz
w[i] := w[i] - learning_rate * dw[i]
db += dz
b := b - learning_rate * dz
while dz[i] = a[i](1-a[i]) (dz here means da/dz, but dL/dz = a-y)
and a = yhat = Sigmoid(z)
J += = -(ylog(yhat) + (1-y)log(1-yhat))
J - J/m

=> Implementing (w, b, X, Y):

import numpy as np
z = np.dot(w.T, X) + b
# compute activation
A = 1 / (1 + np.exp(-z))
# compute cost
cost = - (np.dot(Y, np.log(A.T)) + np.dot((1-Y), np.log(1-A).T)) / m
# backward propagation (to find gradient descent)
dw = np.dot(X, (A - Y).T) / m
dz = A - Y
db = np.sum(dz, axis=1, keepdims=True) / m
# update parameters
w = w - learning_rate * dw
b = b - learning_rate * db

4. Choosing activation function:
Sigmoid (0, 1): good for binary classification
Relu (0, 1) and LeakyRelu (0.01, 1): good as default
Tanh (-1, 1): not recommend
The tanh activation usually works better than sigmoid activation
function for hidden units because the mean of its output is closer
to zero, and so it centers the data better for the next layer.

5. Neural network gradient (backpropagation):
A simple neural network has input layer, hidden layer,
and output layer. It will be 2 layer NN as input layer
is not counted.

# layer [1]
z[1] = W[1]X + b[1]
a[1] = Sigmoid(z[1])
# layer [2]
z[2] = W[2]a[1] + b[2]
a[2] = Sigmoid(z[2])
Loss(a[2], y)

Backpropagation is to update slope W and intercept b,
by taking derivatives, then update them with learning rate

# Implementing backpropagation classification:
# second layer
# dZ2 = A2 - Y
# dW2 = np.dot(dZ2, A1.T) / m
# db2 = np.sum(dZ2, axis=1, keepdims=True) / m
# first layer
# dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
# dW1 = np.dot(dZ1, X.T) / m
# db1 = np.sum(dZ1, axis=1, keepdims=True) / m

dz[2] = a[2] - y = A[2] - Y
dW[2] = dz[2]a[1].T = dZ[2].A[1].T / m
db[2] = np.sum(dZ[2], axis=1, keepdims=True) / m
W[2] = W[2] - learning_rate * dW[2]
b[2] = b[2] - learning_rate * db[2]

dz[1] = W[2].T.dz[2] * g'[1](z[1])
# g'[1](z[1]) = (1 - A1 power of 2)
dW[1] = dz[1].X.T = dZ[1].X.T / m
db[1] = np.sum(dZ[1], axis=1, keepdims=True) / m
W[1] = W[1] - learning_rate * dW[1]
b[1] = b[1] - learning_rate * db[1]

Note: a[2](12) denotes the activation vector of the 2nd
layer for the 12th training example.
X is a matrix in which each column is one training example.
a4[2] is the activation output by the 4th neuron of the 2nd layer.

6. Random initialization:
Should not initializing W with zeros,
because W in all neurons will be the same => no meaning
So should Random initializing W,
for bias, initializing with zeros is OK

W = np.random.randn(n, m) * 0.01 # small is faster and better
# This will cause the inputs of the activation function to also
# be very large, thus causing gradients to be close to zero.
# The optimization algorithm will thus become slow.
b = np.zeros((n, 1))

7. Debuging neuron network:
Simple way is to check shape of output
n is number of neuron of each layer
n[0] is number of neuron of layer 0
m is number of samples

# layer 1
X shape is (n[1], m)
Z[1] shape is (n[1], m)
W[1] shape is (n[1], n[0])
b[1] shape is (n[1], m)

# layer l
W[l] shape is (n[l], n[l-1])
b[l] shape is (n[l], m)
dW[l] shape is (n[l], n[l-1])
db[l] shape is (n[l], m)
dZ[l] shape is (n[l], m)
dA[l] shape is (n[l], m)

In general, the number of neurons in the previous layer gives us
the number of columns of the weight matrix, and the number of
neurons in the current layer gives us the number of rows in the
weight matrix.

8. Hyperparameters:
Hyperparameters can determine values of parameters
W[i] and b[i]. They are:
- learning_rate
- number of iteration
- number of hidden layers
- number of hidden units
- choice of activation functions
- momentum
- batch size
- regulation, and so on

9. Data splitting:
Train/Dev/Test sets

if high bias:
-> train set problem -> try deeper network,
increase the number of units in each hidden layer
if high variance:
-> dev set problem -> get more training data,
increase the regularization parameter lambda
(Weights are pushed toward becoming smaller (closer to 0))


10. Regularization:
L2: lambda * Norm2 of W / 2m
L1: lambda * Norm1 of W / 2m

In neural network:
L2: lambda * Sum of Frobenius Norm2 of Wij[l]
(row i, column j, layer l)

-> L2 regularization can reduce overfitting.
It causes "weight decay", zero out weights.
(W -> 0)

The cost computation with L2 regularization:
  A regularization term is added to the cost
  example:
  cross_entropy_cost = compute_cost(A3, Y)
  L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2))
    + np.sum(np.square(W3))) / (2 * m)
  cost = cross_entropy_cost + L2_regularization_cost

The backpropagation function:
  There are extra terms in the gradients with respect to weight matrices
  example:
  dW1 = 1./m * np.dot(dZ1, X.T) + (lambd * W1 / m)

Weights end up smaller ("weight decay"):
  Weights are pushed to smaller values.

11. Drop out and other regularization methods:
It reduces overfitting.
The whole NN cannot depend on any single neuron.

Other methods are data augmentation, early stopping.

You only use dropout during training. Don't use dropout (randomly eliminate
nodes) during test time.
Apply dropout both during forward:
  example:
  Z1 = np.dot(W1, X) + b1
  A1 = relu(Z1)
  D1 = np.random.rand(A1.shape[0], A1.shape[1]) # Step 1: initialize matrix D1
  D1 = (D1 < keep_prob).astype(int) # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)
  A1 = A1 * D1                      # Step 3: shut down some neurons of A1
  A1 = A1 / keep_prob               # Step 4: scale the value of neurons that haven't been shut down

and backward propagation:
  example:
  dA1 = np.dot(W2.T, dZ2)
  dA1 = dA1 * D1 # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation
  dA1 = dA1 / keep_prob # Step 2: Scale the value of neurons that haven't been shut down

During training time, divide each dropout layer by keep_prob to keep the same
expected value for the activations. For example, if keep_prob is 0.5, then we
will on average shut down half the nodes, so the output will be scaled by 0.5
since only the remaining half are contributing to the solution.
Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has
the same expected value. You can check that this works even when keep_prob is
other values than 0.5.

12. Normalization:
It helps to normalize all features to the same scale,
so training will get faster (faster gradient descent).

Xavier initialization:
Xavier initialization makes sure the weights are â€˜just rightâ€™, keeping the
signal in a reasonable range of values through many layers.

In details, multiply with np.sqrt(1./layers_dims[l-1])

Ref: https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization

He initialization (He et al 2015)
In details, multiply with np.sqrt(2./layers_dims[l-1])

Example:
L = len(layers_dims) - 1 # integer representing the number of layers

for l in range(1, L + 1):
  parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2./layers_dims[l-1])
  parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))

13. Gradient checking:
Use this formula to check value of derivative of Loss J.

d(theta) = (J(theta + epsilon) - J(theta - epsilon)) / 2 epsilon
theta is X (or W, b)
epsilon is a small change of X

To implement this method:
Concatenate all W[i] and b[i] and reshape to create
vector Loss function J(theta[i]).
Run loop over i and take derivative of J by theta[i].

d(theta approx[i]) = (J(theta, ...theta[i] + epsilon, ...) - J(theta, ...theta[i]-epsilon, ...))
devided by 2 epsilon.

Check value of this formula, it calculates euclidean distance
between d(thera approx) and d(theta):
norm2 of (d(theta approx) - d(theta)) / norm2 of d(theta approx) + norm2 of d(theta)

if epsilon is about 10 power of -7, then above formula should give the same value.
if value of above formula bigger than 10 power of -5, or -3, it means that
d(theta approx) is far from d(theta),
then gradient descent was not done well. there is a bug in data of W, and/or b.

This method does not work with Drop out, and not during training,
only for checking gradients.

14. Choosing mini batch size:

batch size = m => Batch gradient descent => too long per iteration
batch size = 1 => stochastic gradient descent = lose speed up from vectorization
batch size between 1, m => faster learning

batch size should be 64, 128, 256, 512, 1024 or x power of 2.
if sample size is small, around 2000 samples, then use Batch gradient descent.

15. Optimization with Exponentially weighted averages (momentum):
Optimization is to speed up gradient descent or training.
It is about caculating trend of moving averages (velocity vector).
v[t] = beta * v[t-1] + (1-beta) * theta[t]

when beta is bigger than 0.5, v[t] depends more on v[t-1], or its previous predicted value.
when beta is smaller than 0.5, v[t] depends more on theta[t] or its real current value.

the higher the beta, the moving average curve is smoother, but delayed more.
the smaller the beta, the moving average curve is more zig zag, noisier, but not delayed.
beta is usually 0.9 (or between 0.8-0.999)

v = 0
loop over m by i:
v := beta * v + (1-beta) * theta[i]

16. Optimization with Bias correction:
v[t] is corrected by this formula

v[t] = v[t] / 1 - beta power of t

when t increases, beta power of t comes close to zero.
so this formula just helps to adjust v[t] when t is small,
or at the beginning.

Vdw = beta * Vdw + (1 - beta)dW
Vdb = beta * Vdb + (1 - beta)db
W = W - alpha * Vdw
b = b - alpha * Vdb

(1-beta)dW, (1-beta)db are to accelerate training, or provide momentum
for gradient descent.

17. Optimization with RMSprop (Root Mean Square prop):
It slows down updating b, but speeds up updating W.

Sdw = beta * Sdw + (1-beta).square(dW)
# we want slope of horizontal slope or dw will be small

Sdb = beta * Sdb + (1-beta).square(db)
# we want db or vertical slop will be large

W := W - alpha (dW/square root of (Sdw + epsilon))
b := b - alpha (db/square root of (Sdb + epsilon))

18. Optimization with Adam (Adaptive Momentum Estimation):

Vdw = beta1.Vdw + (1-beta1)dW
Vdb = beta1.Vdb + (1-beta1)db
Sdw = beta2.Sdw + (1-beta2).square(dW)
Sdb = beta2.Sdb + (1-beta2).square(db)

Vdw = Vdw / 1 - beta1 power of t
Vdb = Vdb / 1 - beta1 power of t
Sdw = Sdw / 1 - beta2 power of t
Sdb = Sdb / 1 - beta2 power of t

W := W - alpha (Vdw / square root of (Sdw + epsilon))
b := b - alpha (Vdb / square root of (Sdb + epsilon))

Hyperparameters:
alpha needs to be tuned
beta1: 0.9  # the first momentum
beta2: 0.999  # the second momentum
epsilon: 10 power of -8

with two momentums => adaptive momentum

19. Learning rate decay:
for example:
learning rata alpha
alpha = alpha * (1/ (decay_rate * epoch))

there are some other ways to optimize decay rate and
learning rate.

20. Local optima vs global optima
Avoid local optima is challenging.
Need to get off plateau and move to global optima
Adam can help to get to global optima faster

21. Adam:

It calculates an exponentially weighted average of past gradients,
and stores it in variables ð‘£ (before bias correction) and ð‘£ ð‘ð‘œð‘Ÿð‘Ÿð‘’ð‘ð‘¡ð‘’ð‘‘
(with bias correction).
It calculates an exponentially weighted average of the squares of the past gradients,
and stores it in variables ð‘  (before bias correction) and ð‘  ð‘ð‘œð‘Ÿð‘Ÿð‘’ð‘ð‘¡ð‘’ð‘‘
(with bias correction).
It updates parameters in a direction based on combining information from calculations above.

Adam paper: https://arxiv.org/pdf/1412.6980.pdf

22. Hyperparameter tuning:

learning rate alpha, momentum beta, number of units in layers, number of layers,
batch size, learning rate decay,...

Pick uniformly random values using an appropriate scale, do not use a grid.
Try coarse to fine search process.
Babysitting one model or training many models in parallel.

23. Batch normalization:

To normalize z[l][i] before applaying activation for each batch.
When input data were changed, it causes "shift distribution".
So batch norm helps to reduce effect of shif distribution by
rescale mini-batch data using its mean/variance.
It adds some noise to the values of z[l][i], similar to dropout.
It has slight regularization effect.

mean miu = sum(z[i]) / m
variance sigma = sum of square(z[i] - mean) / m
z norm[i] = (z[i] - mean) / root square(variance + epsilon)
z tilde[i] = y.z norm[i] + beta

in practice, mean and variance are estimated using exponentially weighted average
across mini-batches

24. Softmax:

a[l] = g[l](z[l]) = np.exp(z[l]) / sum(np.exp(z[l]))

Softmax regression generalizes logistic regression to C classes.
If C = 2, Softmax reduces to logistic regression.

25. Deep Learning frameworks:

Caffe/Caffe2, CNTK, DL4J, Keras, Lasagne, mxnet, PaddlePaddle, TensorFlow,
Theano, Torch

26. ML strategy:

26.1 Orthogonalization:
Choose train/dev/test datasets wisely, logically, randomly, or by time (98%, 1%, 1%)
Single number evaluation metric is better than many metrics
Change or try different train/dev/test sets, other metrics, optimizer,
regularization, cost function, ... when needed.

Need to understand human level performance/error, then compare results
(training and dev/test errors) with human level errors to
choose bias avoidance tactics (bigger models, longer training, better optimization,
decrease regulation,...) or variance avoidance tactics (increase regulation,
bigger dev set, avoid overfit to dev set and training set...).

Human level error
                   | -> bias avoidable tactics (reduce bias)
Training error
                   | -> Variance avoidable tactics (reduce variance)
Dev/Test error

until getting closer or surpass human level error (team of experts, choose the
lowest error)

26.2 Carrying out "error analysis" to evaluate dev set to see quality of data,
count error rate, to find mis-recognized data/labels, to improve blurry images, etc

Example of splitting train/test data sets:
200k of web crawling images
10k of mobile app images
-> 205k for training set, 2.5k of mobile app for dev set,
2.5k of mobile app for test set

-> Avoid mismatched data between training and dev/test sets

When error of dev/test set is much bigger than error of training set, try to
use a training-dev set which is splitted randomly from training set (to have
the same distribution) to test model.
If the error of training-dev test is still bigger than error of training set,
then this is problem of high variance problem, or
problem of the model.

If error of training-dev set is low as error of training set,
then this is problem of data mismatched problem between training,
and dev/test sets.

If error of training set is much bigger than error of human level,
then this is problem of high bias. Should focus on model.

Finally, in the case of artificial data synthesis, just be cautious you might be
accidentally simulating data only from a tiny subset of all possible examples.

26.3 Transfer learning
To retrain a pretrained model with new data for new task

26.4 Multi-task learning
To train one model to do many tasks at the same time, eg: one model to detect
car, trafic light, bike, human, etc

26.5 End-to-end learning
To train one model using the same input and predict directly output, bypassing all
middle (prediction tasks) steps. But this will require lot of data.

In fact, multi-step approach usually makes better result comparing with
end to end learning.

27. Convolutional Neural Networks:

Why convolutions:
- Parameter sharing: a feature detector that's useful in one part of the image
is probably useful in another part of the image.
  -> reduce number of parameters
- Sparsity of connections: in each layer, each output value depends only on a
small number of inputs.

=> reduce overfit

Tips for doing well on benchmarks/winning competitions:
- train several networks independently and average their outputs
- multi-crop at test time
  run classifier on multiple versions of test images and everage results
- use open source code
  use architectures of networks published in the literature
  use open source implementations if possible
  use pretrained models and fine-tune on your datasets

28. Object Localization

How to label object on the image:
y = {Pc, bx, by, bh, bw, C1, C2, C3}
coordinates of top, left point of image is (0, 0)
coordinates of bottom, right point of image is (1, 1)
Pc = 1 if there is object
Pc = 0 if there is no object
(bx, by) is coordinates of center point of the square frame drawing around the object
bh = height value of the square frame
bw = width value of the square frame
C1, C2, C3 are 0 or 1 (one hot) to label the class of the object
(eg, object classes are car, human, cat, dog, trafic light, etc)

add more points on the image of objects => landmark detection

29. Object detection

Using "sliding windows technique", but computation cost and processing time are
problems.

30. YOLO algorigth (You Look Only Once):
- Bounding box predictions:
Divide an image into eg. 3 x 3 = 9 grid cells
the top left point of the grid cell is (0, 0)
the bottom right point of the grid cell is (1, 1)
Label each grid cell for training as bellow
y = {Pc, bx, by, bh, bw, c1, c2, c3} (total 8 numbers or 8 dimentions)
Pc -> 0 or 1 to show there is object to detect or not. It is confidence of an
object being present in the bounding box.
bx, by: center point of the bounding box which has object, bx + by = 1.
bh, bw: is height and width of the bounding box of the object, bh + bw could be
bigger than 1 if the bounding box is bigger than the grid cell (1, 1).
if there is no object in that grid cell, asign ? (meaning don't care)

- For each grid cell, get 2 predicted bounding boxes
- Get rid of low probability predictions (IoU, union of intersection)
- For each class (pedestrian, car, motorcycle) use non-max suppression to generated
final predictions.

The YOLO architecture is: IMAGE (m, 608, 608, 3) -> DEEP CNN -> ENCODING (m, 19,
 19, 5, 85) with 5 anchor boxes, predict 80 classes.

Class score is Pc x ci.
Eg. Probability that an object exists is Pc = 0.6. It means there is 60% chance
that an object exists in the box. The probability that the object is the class i
is ci = 0.73. So the score for the box is 0.6 x 0.73 = 0.44.

Redmon et al 2015 You look only Once: Unified real-time object detection

31. Sementic segmentation with U-Net:
Segmentic segmentation is different with Object detection algorithm.
Locating objects in an image by predicting each pixel as to which class it belongs to.
It will label each pixel on the image to a class. The output is a segmentation map.

32. Transpose Convolution:
From input 2 x 2, apply filter 3 x 3 to create an output 4 x 4 which has padding
p = 1, stride = 2. Note: apply padding and stride to output, not to input.
With the overlap boxes in output, just sum them up to get final value.

33. U-net Architectures:
U-Net, named for its U-shape, was originally created in 2015 for tumor detection,
but in the years since has become a very popular choice for other semantic
segmentation tasks.

input | ------> tranpose convolution --------> | CONV 1 x 1 x nc classes -> output
      | ---------> skip connection ----------> |

Ronneberger et al 2015 U-Net: Convolutional Networks for Biomedical Image segmentation

====================
References:
====================

Wâ€‹eek 1:
The Sequential model (TensorFlow Documentation)
The Functional API (TensorFlow Documentation)

Wâ€‹eek 2:
Deep Residual Learning for Image Recognition (He, Zhang, Ren & Sun, 2015)
dâ€‹eep-learning-models/resnet50.py/ (GitHub: fchollet)
MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Howard, Zhu, Chen, Kalenichenko, Wang, Weyand, Andreetto, â€‹& Adam, 2017)
MobileNetV2: Inverted Residuals and Linear Bottlenecks (Sandler, Howard, Zhu, Zhmoginov &Chen, 2018)
EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan & Le, 2019)

Wâ€‹eek 3:
You Only Look Once: Unified, Real-Time Object Detection (Redmon, Divvala, Girshick & Farhadi, 2015)
YOLO9000: Better, Faster, Stronger (Redmon & Farhadi, 2016)
YAD2K (GitHub: allanzelener)
YOLO: Real-Time Object Detection
Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs (Novikov, Lenis, Major, HladÅ¯vka, Wimmer & BÃ¼hler, 2017)
Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks (Dong, Yang, Liu, Mo & Guo, 2017)
U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger, Fischer & Brox, 2015)

Wâ€‹eek 4:
FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff, Kalenichenko & Philbin, 2015)
DeepFace: Closing the Gap to Human-Level Performance in Face Verification (Taigman, Yang, Ranzato & Wolf)
fâ€‹acenet (GitHub: davidsandberg)
How to Develop a Face Recognition System Using FaceNet in Keras (Jason Brownlee, 2019)
kâ€‹eras-facenet/notebook/tf_to_keras.ipynb (GitHub: nyoki-mtl)
A Neural Algorithm of Artistic Style (Gatys, Ecker & Bethge, 2015)
Convolutional neural networks for artistic style transfer
TensorFlow Implementation of "A Neural Algorithm of Artistic Style"
Vâ€‹ery Deep Convolutional Networks For Large-Scale Image Recognition (Simonyan & Zisserman, 2015)
Pretrained models (MatConvNet)

END!
